{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Julia and JuliaCall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# R is slow, how about writing functions in C/Cpp?\n",
    "\n",
    "## Pros\n",
    "\n",
    "* Faster functions.\n",
    "\n",
    "## Cons\n",
    "\n",
    "* Need to learn a language that is (very) different from R, and is quite low level (C) or quite complicated (cpp).\n",
    "\n",
    "* It doesn't compose well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example from R itself\n",
    "\n",
    "R itself already uses lots of functions in C, but is still slow....\n",
    "\n",
    "R: `sin(sqrt(a))` ## a is some vector\n",
    "\n",
    "1. R: evaluate `sqrt(a)` -> call C function\n",
    "2. C: create a new vector -> do the calculation -> give the result `b = sqrt(a)` back to R\n",
    "3. R: evaluate `sin(b)` -> call C function\n",
    "4. C: create a new vector -> do the calculation -> give the result `c = sin(b)` back to R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a *sub*optimal.\n",
    "\n",
    "Why not\n",
    "1. R: evaluate `sqrt(a)` -> call C function\n",
    "2. C: create a new vector -> do the calculation -> give the result `b = sin(sqrt(a))` back to R?\n",
    "\n",
    "If we want to achive this, we need to write a C function called `sin_sqrt` and then use it from R. And maybe we also need to write `sqrt_sin`, `exp_sin`, `sin_exp`, `cos_sin`, `sin_cos` ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Just In Time (JIT) Compiling\n",
    "\n",
    "* Functions need to be executed will be compiled first.\n",
    "\n",
    "* JIT is widely used, in Julia, in your browser (Google V8), in MATLAB, in Python (numba), and in R.\n",
    "\n",
    "## MATLAB and Python's JIT\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Fast when they work.\n",
    "\n",
    "* Being optimized for much longer than Julia.\n",
    "\n",
    "### Cons\n",
    "\n",
    "* It's limited.\n",
    "\n",
    "* It doesn't compose well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia's advantage\n",
    "\n",
    "Instead of the case in R/Python/MATLAB, where you have the language first, and then consider the optimization (JIT), Julia's design is to make JIT easier.\n",
    "\n",
    "* Not limited: users can define their own types, and Julia will try to do JIT for users.\n",
    "\n",
    "* Compose well: if Julia find further optimization opportunities, it will do it automatically.\n",
    "\n",
    "## An example\n",
    "\n",
    "Julia: `sin.(sqrt.(a))` ## a is some vector\n",
    "\n",
    "1. create a new vector `b`\n",
    "2. for each element `x` in `a` do the calculation of `sin(sqrt(x))` \n",
    "3. stores the result in `b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Julia\n",
    "\n",
    "## Official website\n",
    "\n",
    "<https://julialang.org/>\n",
    "\n",
    "## IDEs\n",
    "\n",
    "* Juno in [Atom](https://atom.io/)\n",
    "\n",
    "* [Visual Studio Code](https://code.visualstudio.com/)\n",
    "\n",
    "* [Jupyter](https://jupyter.org/try)\n",
    "\n",
    "* [Julia Box](https://juliabox.com/)\n",
    "\n",
    "## Version choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Convex optimization: [Convex.jl](https://github.com/JuliaOpt/Convex.jl)\n",
    "\n",
    "* Optimization: [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl/), [JuMP.jl](https://github.com/JuliaOpt/JuMP.jl)\n",
    "\n",
    "* Differential equations: [DifferentialEquations.jl](https://github.com/JuliaDiffEq/DifferentialEquations.jl)\n",
    "\n",
    "* Automatic differentiation: [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl), [ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl)\n",
    "\n",
    "* Mixed Effects Models: [MixedModels.jl](https://github.com/dmbates/MixedModels.jl)\n",
    "\n",
    "* Artificial Intelligence: [Flux.jl](https://github.com/FluxML/Flux.jl), [TensorFlow.jl](https://github.com/malmaud/TensorFlow.jl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.8, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        3\n",
      "\n",
      "Total number of variables............................:        3\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 0.00e+00 0.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "\n",
      "Number of Iterations....: 0\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Dual infeasibility......:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 1\n",
      "Number of objective gradient evaluations             = 1\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.158\n",
      "Total CPU secs in NLP function evaluations           =      0.045\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using Pkg; Pkg.add(\"JuMP\"); Pkg.add(\"Ipopt\")\n",
    "using JuMP\n",
    "using Ipopt\n",
    "\n",
    "p = 3\n",
    "m = Model(solver = IpoptSolver())\n",
    "@variable(m, x[1:p])\n",
    "@objective(m, Min, sum(x.*x))\n",
    "status = solve(m)\n",
    "getvalue(x[1:p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using Pkg; Pkg.add(\"ForwardDiff\")\n",
    "using ForwardDiff\n",
    "\n",
    "function NewtMin(f, x0, eps)\n",
    "    fgrad = x-> ForwardDiff.gradient(f, x)\n",
    "    fhess = x-> ForwardDiff.hessian(f, x)\n",
    "    oldval = f(x0)\n",
    "    newx = x0 - fhess(x0)\\fgrad(x0)\n",
    "    newval = f(newx)\n",
    "    while abs(newval - oldval) > eps\n",
    "        oldval = newval\n",
    "        newx = newx - fhess(newx)\\fgrad(newx)\n",
    "        newval = f(newx)\n",
    "    end\n",
    "    return newx\n",
    "end\n",
    "\n",
    "f(x) = sum(x.^2)\n",
    "\n",
    "NewtMin(f, [1., 1., 1.], 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -5.0823878310249317e-5\n",
       "  9.262030970117879e-6 \n",
       " -1.2828696727829803e-5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using Pkg; Pkg.add(\"Optim\")\n",
    "using Optim\n",
    "\n",
    "r = Optim.optimize(f, [1., 1., 1.])\n",
    "Optim.minimizer(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced: multiple dispatch\n",
    "\n",
    "* Similar to S3 system in R: `print.lm`, `anova.lm`, etc, but works on multiple arguments, and is **fast**!\n",
    "\n",
    "* A simple example:\n",
    "\n",
    "    julia> function f(x::Integer) x+1 end\n",
    "    f (generic function with 1 method)\n",
    "\n",
    "    julia> function f(x::AbstractFloat) 3 * x end\n",
    "    f (generic function with 2 methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More advanced\n",
    "\n",
    "* `@code_llvm`\n",
    "\n",
    "* `@code_warn_type`\n",
    "\n",
    "* `@trace` from [Traceur.jl](https://github.com/MikeInnes/Traceur.jl)\n",
    "\n",
    "## Some more advanced and comprehensive introductions\n",
    "\n",
    "* <https://github.com/johnfgibson/whyjulia>\n",
    "\n",
    "* <http://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# JuliaCall\n",
    "\n",
    "## Website\n",
    "\n",
    "* <https://github.com/Non-Contradiction/JuliaCall>\n",
    "\n",
    "* <https://cran.r-project.org/web/packages/JuliaCall/index.html>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
